{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.PILToTensor(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", train=True, transform=transform, download=True\n",
    ")\n",
    "validation_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_set, batch_size=4, shuffle=False\n",
    ")\n",
    "\n",
    "# Class labels\n",
    "classes = (\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle Boot\",\n",
    ")\n",
    "\n",
    "# Report split sizes\n",
    "print(\"Training set has {} instances\".format(len(training_set)))\n",
    "print(\"Validation set has {} instances\".format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5  # un-normalize\n",
    "    np_img = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(np_img, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "\n",
    "\n",
    "data_iter = iter(training_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print(\"  \".join(classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Vision Transformer Model\n",
    "\n",
    "We will implement:\n",
    "- Positional Encoding ([Visit here for code provided by PyTorch](https://pytorch.org/tutorials/beginner/translation_transformer.html#seq2seq-network-using-transformer) )\n",
    "- Patch Embedding\n",
    "- Vision Transformer Model (using PyTorch Encoder Layer)\n",
    "\n",
    "---\n",
    "\n",
    "## Positional Encoding\n",
    "\n",
    "<img src=\"./assets/positional-encoding.webp\" width=\"900\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float = 0.0, maxlen: int = 5000):\n",
    "        if emb_size % 2 != 0:\n",
    "            raise Exception(\"Embedding size must be even\")\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # fancy logarithmic stuff to finally calculate 10000^(-2i/emb_size)\n",
    "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "\n",
    "        pos_embedding[:, 0::2] = torch.sin(\n",
    "            pos * den\n",
    "        )  # from 0th index to last index with a step of 2\n",
    "        pos_embedding[:, 1::2] = torch.cos(\n",
    "            pos * den\n",
    "        )  # from 1st index to last index with a step of 2\n",
    "        # pos_embedding = pos_embedding.unsqueeze(0) # add a new dimension at the first index, we will use batch_first = True (handle batch dimension)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"pos_embedding\", pos_embedding\n",
    "        )  # register the tensor as buffer - not updated during backprop\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        # no learnable parameters\n",
    "        return self.dropout(token_embedding + self.pos_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Patch Embedding\n",
    "\n",
    "<img src=\"./assets/patch-embedding.png\" width=\"700px\" height=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, patch_size, num_patches, dropout, in_channels):\n",
    "        super().__init__()\n",
    "        self.patcher = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=embed_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,  # take each patch_size pixel\n",
    "            ),\n",
    "            nn.Flatten(2),  # flatten from the 2nd dimension to the end\n",
    "        )\n",
    "\n",
    "        # special classification token\n",
    "        self.special_classification_token = nn.Parameter(\n",
    "            torch.randn(size=(1, in_channels, embed_dim)), requires_grad=True\n",
    "        )\n",
    "\n",
    "        self.position_embeddings = PositionalEncoding(\n",
    "            emb_size=embed_dim, dropout=dropout, maxlen=num_patches + 1\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_token = self.special_classification_token.expand(\n",
    "            x.shape[0], -1, -1\n",
    "        )  # (B, input_channel, E)\n",
    "\n",
    "        x = self.patcher(x).permute(0, 2, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = self.position_embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Vision Transformer Model\n",
    "\n",
    "<img src=\"./assets/ViT.png\" width=\"700px\" height=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        img_size,\n",
    "        num_classes,\n",
    "        patch_size,\n",
    "        embed_dim,\n",
    "        num_encoders,\n",
    "        num_heads,\n",
    "        hidden_dim,\n",
    "        dropout,\n",
    "        activation,\n",
    "        in_channels,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings_block = PatchEmbedding(\n",
    "            embed_dim, patch_size, num_patches, dropout, in_channels\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder_blocks = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_encoders\n",
    "        )\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embed_dim),\n",
    "            nn.Linear(in_features=embed_dim, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings_block(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.mlp_head(x[:, 0, :])  # Apply MLP on the CLS token only\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 10\n",
    "PATCH_SIZE = 4\n",
    "IMG_SIZE = 28\n",
    "IN_CHANNELS = 1\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.001\n",
    "HIDDEN_DIM = 768\n",
    "ADAM_WEIGHT_DECAY = 0\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "ACTIVATION = \"gelu\"\n",
    "NUM_ENCODERS = 4\n",
    "EMBED_DIM = (PATCH_SIZE**2) * IN_CHANNELS  # 16\n",
    "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2  # 49\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dummy data and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    NUM_PATCHES,\n",
    "    IMG_SIZE,\n",
    "    NUM_CLASSES,\n",
    "    PATCH_SIZE,\n",
    "    EMBED_DIM,\n",
    "    NUM_ENCODERS,\n",
    "    NUM_HEADS,\n",
    "    HIDDEN_DIM,\n",
    "    DROPOUT,\n",
    "    ACTIVATION,\n",
    "    IN_CHANNELS,\n",
    ").to(device)\n",
    "x = torch.randn(512, 1, 28, 28).to(device)\n",
    "print(model(x).shape)  # BATCH_SIZE X NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Now, let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# values from the paper\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    betas=ADAM_BETAS,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=ADAM_WEIGHT_DECAY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_one_epoch():\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs.to(device))\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000  # loss per batch\n",
    "            print(\"  batch {} loss: {}\".format(i + 1, last_loss))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5  # good enough for testing, also dataset is small\n",
    "my_loss_curve = []\n",
    "\n",
    "best_v_loss = 1_000_000.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH {}:\".format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch()\n",
    "\n",
    "    running_v_loss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, v_data in enumerate(validation_loader):\n",
    "            v_inputs, v_labels = v_data\n",
    "            v_outputs = model(v_inputs.to(device))\n",
    "            v_loss = loss_fn(v_outputs, v_labels.to(device))\n",
    "            running_v_loss += v_loss.item()\n",
    "\n",
    "    avg_v_loss = running_v_loss / (i + 1)\n",
    "    print(\"LOSS train {} valid {}\".format(avg_loss, avg_v_loss))\n",
    "\n",
    "    my_loss_curve.append((avg_loss, avg_v_loss))\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_v_loss < best_v_loss:\n",
    "        print(\"-\" * 80)\n",
    "        print(\"  New best loss! Saving model.\")\n",
    "        best_v_loss = avg_v_loss\n",
    "        model_path = \"models/best-model-trained.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### let's plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the loss curve\n",
    "train_losses = [x[0] for x in my_loss_curve]\n",
    "valid_losses = [x[1] for x in my_loss_curve]\n",
    "\n",
    "plt.plot(train_losses, label=\"Training loss\")\n",
    "plt.plot(valid_losses, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualize prediction\n",
    "\n",
    "<img src=\"./assets/prediction-by-ViT.png\" height=\"400\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_my_img(my_img, my_label, pred, ax, j):\n",
    "    row = j // 2\n",
    "    col = j % 2\n",
    "\n",
    "    ax[row, col].imshow(my_img.permute(1, 2, 0))  # permute to (H, W, C)\n",
    "    ax[row, col].axis(\"off\")\n",
    "    if my_label == pred:\n",
    "        ax[row, col].set_title(classes[my_label], color=\"green\")\n",
    "    else:\n",
    "        ax[row, col].set_title(classes[pred] + \"=> \" + classes[my_label], color=\"red\")\n",
    "\n",
    "\n",
    "for i in validation_loader:\n",
    "    v_inputs, v_labels = i\n",
    "\n",
    "    v_outputs = model(v_inputs.to(device))\n",
    "    my_preds = [x.argmax().item() for x in v_outputs]\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "    for j in range(len(my_preds)):\n",
    "        show_my_img(v_inputs[j], v_labels[j], my_preds[j], ax, j)\n",
    "    plt.show()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
